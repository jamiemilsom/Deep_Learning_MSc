{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from functools import partial\n",
    "\n",
    "import ray.cloudpickle as pickle\n",
    "import ray\n",
    "from ray import train,tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Running on device: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "print('Cuda is available:',torch.cuda.is_available())\n",
    "print('Running on device:',torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Training Data\n",
    "\n",
    "The Position-Specific Scoring Matrix (PSSM) encodes the likelihood of observing each amino acid at every position within a protein sequence, derived from multiple sequence alignments. In our dataset, the PSSM probability values are already formatted appropriately for training. They span a range from 0 to 1, where 0 indicates no likelihood and 1 signifies absolute likelihood. \n",
    "\n",
    "The secondary structure of a protein is influenced by a combination of the amino acid sequence, local interactions, and the overall folding landscape of the protein. Using convolutional layers to assess the local interactions should allow the model to predict the secondry stucture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of arbitary test sequence 413\n",
      "Length of arbitary train sequence 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RES_NUM</th>\n",
       "      <th>AMINO_ACID</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>G</td>\n",
       "      <td>0.421277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316258</td>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>0.018557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303780</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109966</td>\n",
       "      <td>0.091409</td>\n",
       "      <td>0.050172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.749662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098106</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.090663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>409</td>\n",
       "      <td>M</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018138</td>\n",
       "      <td>0.087666</td>\n",
       "      <td>0.187424</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.008464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>410</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048632</td>\n",
       "      <td>0.903951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>411</td>\n",
       "      <td>T</td>\n",
       "      <td>0.338152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037084</td>\n",
       "      <td>0.257071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.164048</td>\n",
       "      <td>0.106223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>412</td>\n",
       "      <td>W</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>413</td>\n",
       "      <td>F</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630915</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RES_NUM AMINO_ACID         A    C         D         E         F  \\\n",
       "0          1          S  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "1          2          G  0.421277  0.0  0.000000  0.000000  0.000000   \n",
       "2          3          F  0.000000  0.0  0.000000  0.000000  0.305865   \n",
       "3          4          E  0.018557  0.0  0.303780  0.319588  0.000000   \n",
       "4          5          F  0.000000  0.0  0.000000  0.000000  0.749662   \n",
       "..       ...        ...       ...  ...       ...       ...       ...   \n",
       "408      409          M  0.209190  0.0  0.000000  0.000000  0.132406   \n",
       "409      410          E  0.000000  0.0  0.048632  0.903951  0.000000   \n",
       "410      411          T  0.338152  0.0  0.000000  0.000000  0.000000   \n",
       "411      412          W  0.000000  0.0  0.000000  0.000000  0.015152   \n",
       "412      413          F  0.000000  0.0  0.000000  0.000000  0.369085   \n",
       "\n",
       "            G         H         I  ...         M         N    P         Q  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
       "1    0.578723  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
       "2    0.000000  0.000000  0.087602  ...  0.000000  0.000000  0.0  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.062543  0.0  0.017182   \n",
       "4    0.000000  0.000000  0.000000  ...  0.009472  0.000000  0.0  0.000000   \n",
       "..        ...       ...       ...  ...       ...       ...  ...       ...   \n",
       "408  0.000000  0.000000  0.022370  ...  0.227932  0.000000  0.0  0.000000   \n",
       "409  0.000000  0.000000  0.000000  ...  0.000000  0.047416  0.0  0.000000   \n",
       "410  0.000000  0.037084  0.257071  ...  0.027027  0.000000  0.0  0.000000   \n",
       "411  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
       "412  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.000000   \n",
       "\n",
       "       R         S         T         V         W         Y  \n",
       "0    0.0  1.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2    0.0  0.000000  0.000000  0.316258  0.123237  0.000000  \n",
       "3    0.0  0.109966  0.091409  0.050172  0.000000  0.000000  \n",
       "4    0.0  0.000000  0.000000  0.098106  0.006089  0.090663  \n",
       "..   ...       ...       ...       ...       ...       ...  \n",
       "408  0.0  0.018138  0.087666  0.187424  0.045345  0.008464  \n",
       "409  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "410  0.0  0.025141  0.164048  0.106223  0.000000  0.045255  \n",
       "411  0.0  0.000000  0.000000  0.000000  0.984848  0.000000  \n",
       "412  0.0  0.000000  0.000000  0.000000  0.630915  0.000000  \n",
       "\n",
       "[413 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_test = pd.read_csv('test/1A0S_1_P.csv')\n",
    "single_train = pd.read_csv('train/1A0A_3_A.csv')\n",
    "print('Length of arbitary test sequence',len(single_test))\n",
    "print('Length of arbitary train sequence',len(single_train))\n",
    "single_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the data that they are 20 different amino acids for each position in the sequence, we can also observe that the length of the sequences vary between these two arbitary proteins. Diving a bit deeer we can observe that the maximum sequence length for our dataset is 1733."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_dir, file_name)\n\u001b[0;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     line_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[1;32m     11\u001b[0m     line_counts_train\u001b[38;5;241m.\u001b[39mappend(line_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:624\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1966\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1964\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[0;32m-> 1966\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m--> 443\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:536\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    534\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(data):\n\u001b[0;32m--> 536\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:654\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_mgr, s\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:5133\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5116\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5117\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   5118\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5131\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 5133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:5592\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m   5591\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 5592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentical\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5595\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   5596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   5597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   5599\u001b[0m \u001b[38;5;66;03m# check if we are a multi reindex\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:5593\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m   5591\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m-> 5593\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentical\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5594\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis_name, ax \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5596\u001b[0m ):\n\u001b[1;32m   5597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   5599\u001b[0m \u001b[38;5;66;03m# check if we are a multi reindex\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5671\u001b[0m, in \u001b[0;36mIndex.identical\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5647\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midentical\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   5649\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5650\u001b[0m \u001b[38;5;124;03m    Similar to equals, but checks that object attributes and types are also equal.\u001b[39;00m\n\u001b[1;32m   5651\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5668\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   5669\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   5670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 5671\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5672\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   5673\u001b[0m             \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, c, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, c, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   5674\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comparables\n\u001b[1;32m   5675\u001b[0m         )\n\u001b[1;32m   5676\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(other)\n\u001b[1;32m   5677\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   5678\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5618\u001b[0m, in \u001b[0;36mIndex.equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(other):\n\u001b[1;32m   5614\u001b[0m     \u001b[38;5;66;03m# quickly return if the lengths are different\u001b[39;00m\n\u001b[1;32m   5615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m-> 5618\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m, StringDtype)\n\u001b[1;32m   5619\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow_numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5620\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   5621\u001b[0m ):\n\u001b[1;32m   5622\u001b[0m     \u001b[38;5;66;03m# special case for object behavior\u001b[39;00m\n\u001b[1;32m   5623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m))\n\u001b[1;32m   5625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(other\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   5626\u001b[0m     \u001b[38;5;66;03m# if other is not object, use other's logic for coercion\u001b[39;00m\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:974\u001b[0m, in \u001b[0;36mIndex.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(result, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 974\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DtypeObj:\n\u001b[1;32m    976\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;124;03m    Return the dtype object of the underlying data.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03m    dtype('int64')\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dir = 'train'\n",
    "line_counts_train = []\n",
    "test_dir = 'test'\n",
    "line_counts_test = []\n",
    "\n",
    "for file_name in os.listdir(train_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(train_dir, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        line_count = len(df)\n",
    "        line_counts_train.append(line_count)\n",
    "        \n",
    "for file_name in os.listdir(test_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(test_dir, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        line_count = len(df)\n",
    "        line_counts_test.append(line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(max(line_counts_train),max(line_counts_test)) \n",
    "min_sequence_length = min(min(line_counts_train),min(line_counts_test))\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.subplot(2,1,1)\n",
    "sns.histplot(line_counts_train, kde=True)\n",
    "plt.xlim(0,max_sequence_length)\n",
    "plt.title('Train Data Sequence Lengths')\n",
    "plt.tight_layout()\n",
    "plt.subplot(2,1,2)\n",
    "sns.histplot(line_counts_test, kde=True)\n",
    "plt.xlim(0,max_sequence_length)\n",
    "plt.title('Test Data Sequence Lengths')\n",
    "plt.tight_layout()  \n",
    "\n",
    "print('minimum sequence length:',min_sequence_length)\n",
    "print('maximum sequence length:',max_sequence_length)\n",
    "print('max test length:',max(line_counts_test))\n",
    "print('max train length:',max(line_counts_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising both the train and test length to compare distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(line_counts_train)):\n",
    "    if i >= len(line_counts_test):\n",
    "        line_counts_test.append(np.nan)\n",
    "\n",
    "train_test_df = pd.DataFrame({'train':line_counts_train,'test':line_counts_test})\n",
    "train_test_df\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.histplot(train_test_df['train'],stat='density',kde=True,label='Train')\n",
    "sns.histplot(train_test_df['test'],stat='density',kde=True,label='Test')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency Density')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Processing and Feature Selection\n",
    "\n",
    "### One hot encode or use probabilities?\n",
    "In terms of preprocessing for the nueral network, although one hot encoding of the amino acid could be used, as it has the same dimensionality as the probabilities and less information, therefore the probabilities will be used. This will result in a 20 channel (number of different amino acids) by max_sequence_length size input layer. \n",
    "\n",
    "i.e shape (N, 20, max_sequence_length) where N is the batch size\n",
    "\n",
    "### Convolution to observe local interactions\n",
    "Using a conv1D kernel, the network can hopefully learn what the local interactions between the amino acid percentages signify, before passing the data onto some dense layers.\n",
    "\n",
    "Finally as the training data is in a different order by PBD_ID to the corresponding labels, i will quickly reorder it to mean that the dataset is simpler to create.\n",
    "\n",
    "### Removing sequence data after length 750\n",
    "As making the input layer the max length of the training data would result in an input layer of 1750, instead taking the max length of the train data allows lower computational requirements.\n",
    "\n",
    "We will experiment to see if accuracy is lost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'test'\n",
    "train_dir = 'train'\n",
    "\n",
    "\n",
    "for file_name in os.listdir(test_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        new_file_name = file_name.replace('_test', '')\n",
    "        os.rename(os.path.join(test_dir, file_name), os.path.join(test_dir, new_file_name))\n",
    "\n",
    "for file_name in os.listdir(train_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        new_file_name = file_name.replace('_train', '')\n",
    "        os.rename(os.path.join(train_dir, file_name), os.path.join(train_dir, new_file_name))\n",
    "        \n",
    "\n",
    "\n",
    "df = pd.read_csv('labels_train.csv')\n",
    "df_sorted = df.sort_values('PDB_ID')\n",
    "df_sorted.to_csv('labels_train_sorted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding the output for training data and creating a reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_sequence(sequence, max_sequence_length):\n",
    "    encoding = {'H': [1,0,0,0], 'E': [0,1,0,0], 'C': [0,0,1,0], ' ': [0,0,0,1]}\n",
    "    encoded_sequence = []\n",
    "\n",
    "    if len(sequence) > max_sequence_length:\n",
    "        sequence = sequence[:max_sequence_length]\n",
    "        \n",
    "    for char in sequence:\n",
    "        encoded_sequence.extend(encoding[char])\n",
    "    \n",
    "    print(len(sequence))\n",
    "    print(max_sequence_length)\n",
    "        \n",
    "    num_blanks = max_sequence_length - len(sequence)\n",
    "    encoded_sequence.extend([0, 0, 0, 1] * num_blanks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return torch.tensor(encoded_sequence, dtype=torch.float32).reshape(max_sequence_length,4).T\n",
    "\n",
    "\n",
    "def unencode_output(encoded_sequence, threshold=0.5):\n",
    "    # the encoded_sequence is a N x 4 x L torch.tensor\n",
    "    \n",
    "    sequence = ''\n",
    "    \n",
    "    for i in range(encoded_sequence.size(2)):  \n",
    "        \n",
    "        if encoded_sequence[0, 0, i] >= threshold:\n",
    "            sequence += 'H'\n",
    "        elif encoded_sequence[0, 1, i] >= threshold:\n",
    "            sequence += 'E'\n",
    "        elif encoded_sequence[0, 2, i] >= threshold:\n",
    "            sequence += 'C'\n",
    "        else:\n",
    "            sequence += ' '\n",
    "    return sequence\n",
    "\n",
    "\n",
    "test_string = 'HCEC HCCCEC H'\n",
    "test_encoded = one_hot_encode_sequence(test_string, 7)\n",
    "print('Original:', test_string)\n",
    "print('Encoded:', test_encoded)\n",
    "print('Encoded shape:', test_encoded.shape)\n",
    "test_encoded.unsqueeze_(0)\n",
    "print('Decoded:', unencode_output(test_encoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data as the PSSM data\n",
    "\n",
    "We need to load 750 rows of 20 probabilites from the file train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSSM_speedy(Dataset):\n",
    "    \"\"\"Position-Specific Scoring Matrix (PSSM) Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, pssm_root_dir, sec_struct_file_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pssm_root_dir (string): Directory with all the protein PSSM data.\n",
    "            sec_struct_root_dir (string): Directory with all the protein secondary structure data. \n",
    "            transform (callable, optional): Optional transform to be applied on a sample. (not used here)           \n",
    "        \"\"\"\n",
    "        self.pssm_root_dir = pssm_root_dir\n",
    "        self.sec_struct_file_path = sec_struct_file_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.file_list = [file for file in os.listdir(pssm_root_dir) if file.endswith('.csv')]\n",
    "        self.tensor_files_dir = os.path.join(pssm_root_dir, 'tensor_files_750')\n",
    "        os.makedirs(self.tensor_files_dir, exist_ok=True)\n",
    "\n",
    "    def preprocess_and_save_tensors(self): # Depending on system requirements may be memory intensive to run\n",
    "        \n",
    "        for file_name in self.file_list:\n",
    "            \n",
    "            pssm_csv_file = os.path.join(self.pssm_root_dir, file_name)\n",
    "            pbd_id = os.path.splitext(file_name)[0]\n",
    "            pbd_id = ''.join(pbd_id)\n",
    "\n",
    "            pssm_tensor_file_path = os.path.join(self.tensor_files_dir, f\"{pbd_id}.pt\")\n",
    "            \n",
    "            if os.path.exists(pssm_tensor_file_path):\n",
    "                continue\n",
    "            else:\n",
    "                pssm_data = pd.read_csv(pssm_csv_file)\n",
    "                pssm_data = pssm_data.drop(columns=['RES_NUM', 'AMINO_ACID'], axis=1)\n",
    "                \n",
    "                num_zeros = 0\n",
    "                if len(pssm_data) > max_sequence_length:\n",
    "                    pssm_data = pssm_data[:max_sequence_length]\n",
    "                else:               \n",
    "                    num_zeros = max_sequence_length - len(pssm_data)\n",
    "                if num_zeros > 0:\n",
    "                    df_zeros = pd.DataFrame(0, index=range(num_zeros), columns=pssm_data.columns)\n",
    "                    pssm_data = pd.concat([pssm_data, df_zeros], ignore_index=True)\n",
    "                \n",
    "                pssm_data = pssm_data.T.to_numpy()\n",
    "                pssm_data = torch.tensor(pssm_data, dtype=torch.float32)\n",
    "\n",
    "                torch.save(pssm_data, pssm_tensor_file_path)\n",
    "            \n",
    "            \n",
    "            if self.sec_struct_file_path is not None:\n",
    "                \n",
    "                sec_struct_tensor_file_path = os.path.join(self.tensor_files_dir, f\"{pbd_id}_sec_struct.pt\")\n",
    "                if os.path.exists(sec_struct_tensor_file_path):\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    sec_struct_data = pd.read_csv(self.sec_struct_file_path)\n",
    "                        \n",
    "                    sec_struc_value = sec_struct_data.loc[sec_struct_data['PDB_ID'] == pbd_id, 'SEC_STRUCT'].values[0]\n",
    "                    encoded_sec_struc = one_hot_encode_sequence(sec_struc_value, max_sequence_length)\n",
    "\n",
    "                    torch.save(encoded_sec_struc, sec_struct_tensor_file_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        pbd_id = os.path.splitext(file_name)[0]\n",
    "        pbd_id = ''.join(pbd_id)\n",
    "        tensor_file_path = os.path.join(self.tensor_files_dir, f\"{pbd_id}.pt\")\n",
    "        pssm_data = torch.load(tensor_file_path)\n",
    "        input = pssm_data\n",
    "\n",
    "        \n",
    "        if self.sec_struct_file_path is not None:\n",
    "\n",
    "            sec_struct_tensor_file_path = os.path.join(self.tensor_files_dir, f\"{pbd_id}_sec_struct.pt\")\n",
    "            encoded_sec_struc = torch.load(sec_struct_tensor_file_path)\n",
    "            target = torch.argmax(encoded_sec_struc,dim=0)\n",
    "            return input, target\n",
    "        \n",
    "        else:\n",
    "            return pbd_id,input\n",
    "\n",
    "\n",
    "\n",
    "speedy_dataset = PSSM_speedy(pssm_root_dir='/home/jamiemilsom/deep_learning/Deep_Learning_MSc/Kaggle/PSSM_Profiles/train',\n",
    "                      sec_struct_file_path='/home/jamiemilsom/deep_learning/Deep_Learning_MSc/Kaggle/PSSM_Profiles/labels_train.csv')\n",
    "\n",
    "test_dataset = PSSM_speedy(pssm_root_dir='/home/jamiemilsom/deep_learning/Deep_Learning_MSc/Kaggle/PSSM_Profiles/test',\n",
    "                      sec_struct_file_path=None)\n",
    "\n",
    "#test_dataset.preprocess_and_save_tensors()\n",
    "speedy_dataset.preprocess_and_save_tensors()\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "train_size = int(0.65 * len(speedy_dataset))\n",
    "val_size = len(speedy_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(speedy_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=64, shuffle=False)\n",
    "train_and_val_dataloader = DataLoader(speedy_dataset,batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly check the speed to run through a whole epoch with no forward or backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for input, target in train_dataloader:\n",
    "    continue\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the CNN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BatchNorm1d\n",
    "\n",
    "\n",
    "dropout_prob = 0.3\n",
    "weight_decay = 1e-4\n",
    "lr=1e-5\n",
    "momentum=0\n",
    "dampening=0\n",
    "num_classes = 4\n",
    "max_sequence_length = 1733\n",
    "pssm_input_channels = 20\n",
    "\n",
    "class ProteinStructurePredictor(nn.Module):\n",
    "    def __init__(self,dropout_prob,k1,k2,k3,l1,c1,c2,c3,num_classes,max_sequence_length=max_sequence_length): # k = kernel sizes, c = channels,l = linear layers\n",
    "        super(ProteinStructurePredictor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=pssm_input_channels, out_channels=c1, kernel_size=k1, padding=((k1-1)//2)), # written long hand first time to help me remeber the order of the arguments\n",
    "            nn.ReLU(),\n",
    "            BatchNorm1d(c1),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Conv1d(c1, c2, kernel_size=k2, padding=((k2-1)//2)),\n",
    "            nn.ReLU(),\n",
    "            BatchNorm1d(c2),\n",
    "            nn.Conv1d(c2, c3, kernel_size=k3, padding=((k3-1)//2)),\n",
    "            nn.ReLU(),\n",
    "            BatchNorm1d(c3)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(c3 * max_sequence_length, l1 * max_sequence_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),          \n",
    "            nn.Linear(l1 * max_sequence_length, num_classes * max_sequence_length),\n",
    "        )\n",
    "        self.max_seq_length = max_sequence_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        x = x.view(-1, 4, self.max_seq_length)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = ProteinStructurePredictor(dropout_prob=dropout_prob,k1=7,k2=5,k3=3,l1=6,c1=4,c2=4,c3=4,num_classes=num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_conv = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool ok now they are in the same format :)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, loss_fn, optimizer, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total_train += labels.size(0) * labels.size(1) - (labels == 3).sum().item()\n",
    "            \n",
    "            correct_train += torch.sum((predicted == labels)).item() - (labels == 3).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                total_val += labels.size(0) * labels.size(1) - (labels == 3).sum().item()\n",
    "                correct_val += torch.sum((predicted == labels)).item() - (labels == 3).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_dataloader, val_dataloader, loss_fn, optimizer_conv, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize = (18,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Raytune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ray.init()\n",
    "print(context.dashboard_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_raytune(config):\n",
    "    model = ProteinStructurePredictor(dropout_prob=config[\"dropout_prob\"],k1=config[\"k1\"],k2=config[\"k2\"],k3=config[\"k3\"],l1=config[\"l1\"],c1=config[\"c1\"],c2=config[\"c2\"],c3=config[\"c3\"],num_classes=4)\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    checkpoint = train.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        with train.get_checkpoint().as_directory() as checkpoint_dir:\n",
    "          with open(os.path.join(checkpoint_dir, 'data.pkl'), 'rb') as fp:\n",
    "            checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    train_subset, val_subset = train_dataset, val_dataset\n",
    "\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0) # maybe change this to just loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "                \n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0) * labels.size(1) - (labels == 3).sum().item()\n",
    "                correct += torch.sum((predicted == labels)).item() - (labels == 3).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy() * inputs.size(0)\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "          with open(os.path.join(checkpoint_dir, 'data.pkl'), 'wb') as fp:\n",
    "            pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "          checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "\n",
    "          train.report(\n",
    "              {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "              checkpoint=checkpoint,\n",
    "          )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_accuracy(model, device=device):\n",
    "    trainset, testset = test_dataset, val_dataset\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=32, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0) * labels.size(1) - (labels == 3).sum().item()\n",
    "            correct += torch.sum((predicted == labels)).item() - (labels == 3).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "\n",
    "    config = {\n",
    "        \"l1\": tune.choice([4,5,6]),\n",
    "        \"dropout_prob\": tune.uniform(0, 0.7),\n",
    "        \"k1\": tune.choice([3, 5, 7]),\n",
    "        \"k2\": tune.choice([3, 5, 7]),\n",
    "        \"k3\": tune.choice([3, 5, 7]),\n",
    "        \"c1\": tune.choice([4, 8, 16]),\n",
    "        \"c2\": tune.choice([4, 8, 16]),\n",
    "        \"c3\": tune.choice([4, 8, 16]),\n",
    "        \"batch_size\": tune.choice([16]),\n",
    "        \"weight_decay\": tune.loguniform(1e-4, 1e-1),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=10,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    result = tune.run(\n",
    "        train_model_raytune,\n",
    "        resources_per_trial={\"cpu\": 16, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        max_concurrent_trials=2,\n",
    "    )\n",
    "    \n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = ProteinStructurePredictor(best_trial.config[\"dropout_prob\"],best_trial.config[\"k1\"],best_trial.config[\"k2\"],best_trial.config[\"k3\"],best_trial.config[\"l1\"],best_trial.config[\"c1\"],best_trial.config[\"c2\"],best_trial.config[\"c3\"],num_classes=4)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = best_trial.checkpoint;\n",
    "\n",
    "    if best_checkpoint:\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "          with open(os.path.join(checkpoint_dir, 'data.pkl'), 'rb') as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "            best_trained_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "            test_acc = test_accuracy(best_trained_model, device)\n",
    "            print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(num_samples=10, max_num_epochs=50, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on both joint train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_and_val_dataloader, val_dataloader, loss_fn, optimizer_conv, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize = (18,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "all_pbids = []\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "val_loss = 0.0\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "with torch.no_grad():\n",
    "    for pbid, inputs in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_pbids.extend(pbid)\n",
    "        all_outputs.append(outputs)\n",
    "all_outputs_tensor = torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_outputs_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unencode_submission(encoded_sequence):\n",
    "    # the encoded_sequence is a N x 4 x L torch.tensor\n",
    "    # encoding = {'H': [1,0,0,0], 'E': [0,1,0,0], 'C': [0,0,1,0], ' ': [0,0,0,1]}\n",
    "    \n",
    "    sequence = ''\n",
    "    \n",
    "    for i in range(encoded_sequence.size(0)):  \n",
    "        \n",
    "        if encoded_sequence[i] == 0:\n",
    "            sequence += 'H'\n",
    "        elif encoded_sequence[i] == 1:\n",
    "            sequence += 'E'\n",
    "        elif encoded_sequence[i] == 2:\n",
    "            sequence += 'C'\n",
    "        elif encoded_sequence[i] == 3:\n",
    "            sequence += ' '\n",
    "        else:\n",
    "            sequence += 'X'\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Count the number of lines in the file\n",
    "        count = sum(1 for line in f)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(pbids, output_tensors, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"ID,STRUCTURE\\n\")\n",
    "        for i, pbid in enumerate(pbids):\n",
    "            output = output_tensors[i,:,:]\n",
    "            sequence = unencode_submission(torch.argmax(output, dim=0))\n",
    "\n",
    "            for j in range(count_rows_in_file('test/' + pbid + '.csv') - 1):\n",
    "                f.write(f\"{pbid}_{j+1},{sequence[j]}\\n\")\n",
    "\n",
    "\n",
    "output_file = \"submission.csv\"\n",
    "create_submission(all_pbids, all_outputs_tensor.cpu(), output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file should be 43161 + header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = \"submission.csv\"\n",
    "num_rows = count_rows_in_file(submission_file)\n",
    "print(\"Number of rows in the submission file:\", num_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
